{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MSM",
      "provenance": [],
      "collapsed_sections": [
        "4-oj50OfQRTJ",
        "7Zc6a7hVQfEx",
        "uqzf3laLQnKT"
      ],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMeeyYo4fZ7qAtdVHD/s5qN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Diyarmo/MSM_predictor/blob/master/MSM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-oj50OfQRTJ",
        "colab_type": "text"
      },
      "source": [
        "# Updating Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ettyarOsJSn",
        "colab_type": "code",
        "outputId": "fbb50960-1fe7-4e9b-91d7-17b69af7373a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install tensorflow==2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==2 in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2) (2.0.1)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2) (0.2.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2) (1.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2) (1.11.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2) (1.17.5)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2) (0.8.1)\n",
            "Requirement already satisfied: tensorboard<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2) (2.0.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2) (0.33.6)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2) (1.0.8)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2) (0.1.8)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2) (0.9.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2) (3.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2) (3.10.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2) (3.1.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2) (1.11.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2) (2.21.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2) (0.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2) (0.16.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2) (42.0.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2) (2.8.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2) (4.0.0)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2) (0.2.7)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2) (1.24.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2) (1.3.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Zc6a7hVQfEx",
        "colab_type": "text"
      },
      "source": [
        "# Connecting To Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8jnFXpC3zgV",
        "colab_type": "code",
        "outputId": "e2db90a6-402a-4510-e1b3-39b72ff758d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqzf3laLQnKT",
        "colab_type": "text"
      },
      "source": [
        "# Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HV0u8gRs4j-h",
        "colab_type": "code",
        "outputId": "9da6c56c-2e63-40dd-d7ba-b422cf886350",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras import metrics\n",
        "from keras import callbacks\n",
        "from keras import regularizers\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Py7ZOBv8QvZF",
        "colab_type": "text"
      },
      "source": [
        "# Opening and Cleaning \"data.csv\" and \"test.csv\" files\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gx3YI7deRXjf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_and_clean_file(filename):\n",
        "  data = pd.read_csv(filename)\n",
        "  data = data.drop([\"Unnamed: 0\", \"Name of show\"], axis=1) #\"Name of show\" and \"Episode\" are same\n",
        "\n",
        "  data['Start_time'] = data['Start_time'].str[11:]\n",
        "  data[\"Start_time\"] = (data[\"Start_time\"].str[0:2].astype(float)) + (data[\"Start_time\"].str[3:5].astype(float))/60\n",
        "  data[\"Length\"] = data[\"Length\"]/4\n",
        "  data = data.drop([\"End_time\"], axis=1)\n",
        "\n",
        "  data[\"Year\"] = data[\"Year\"].astype(str)\n",
        "  data[\"Month\"] = data[\"Date\"].str[5:7]\n",
        "  data[\"Day\"] = data[\"Date\"].str[8:10].astype(np.int8)\n",
        "  data = data.drop([\"Date\"], axis=1)\n",
        "\n",
        "  data = data.drop([\"Name of episode\"], axis=1) #Not using these feature\n",
        "\n",
        "  for col in [\"First time or rerun\", \"# of episode in the season\", \"Movie?\", \"Game of the Canadiens during episode?\"]: \n",
        "    data[col] = (data[col] == \"Yes\").astype(np.int8)\n",
        "\n",
        "  return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dhO9BSh9L5J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = read_and_clean_file(\"drive/My Drive/data.csv\")\n",
        "test = read_and_clean_file(\"drive/My Drive/test.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mSxssu6CUYSX"
      },
      "source": [
        "# Filling NA values in data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEiEFa7_2mZB",
        "colab_type": "code",
        "outputId": "d8b79636-fe38-4e24-a524-c15489ac5c76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        }
      },
      "source": [
        "print(data.isnull().sum())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode                                       0\n",
            "Station                                       0\n",
            "Channel Type                                  0\n",
            "Season                                        0\n",
            "Year                                          0\n",
            "Day of week                                   0\n",
            "Start_time                                   43\n",
            "Length                                        0\n",
            "Genre                                         0\n",
            "First time or rerun                           0\n",
            "# of episode in the season                    0\n",
            "Movie?                                        0\n",
            "Game of the Canadiens during episode?         0\n",
            "Market Share_total                            0\n",
            "Temperature in Montreal during episode    83344\n",
            "Month                                         0\n",
            "Day                                           0\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTPzfcO_NcGL",
        "colab_type": "text"
      },
      "source": [
        "**Filling missing data in Start time using**\n",
        "\n",
        "1- Mean of Start time\n",
        "\n",
        "\n",
        "**Filling missing data in Temperature using**\n",
        "\n",
        "1- Temperature of that time if found in other rows\n",
        "\n",
        "2- Mean temperature of that day"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ik2x75MZj6cF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fill_NAs(data):\n",
        "  data[\"Start_time\"] = data[\"Start_time\"].fillna(np.round(data[\"Start_time\"].mean()*2)/2)\n",
        "\n",
        "  Temp = pd.DataFrame(data.groupby( [\"Year\", \"Month\", \"Day\", \"Start_time\"])[\"Temperature in Montreal during episode\"].mean())\n",
        "  NanIndex = data[\"Temperature in Montreal during episode\"][data[\"Temperature in Montreal during episode\"].isnull()].index\n",
        "  for i in NanIndex:\n",
        "    d = data.loc[i]\n",
        "    data.at[i, \"Temperature in Montreal during episode\"] = Temp.loc[d[\"Year\"], d[\"Month\"], d[\"Day\"], d[\"Start_time\"]].values[0]\n",
        "\n",
        "  Temp = pd.DataFrame(data.groupby( [\"Year\", \"Month\", \"Day\"])[\"Temperature in Montreal during episode\"].mean())\n",
        "  NanIndex = data[\"Temperature in Montreal during episode\"][data[\"Temperature in Montreal during episode\"].isnull()].index\n",
        "  for i in NanIndex:\n",
        "    d = data.loc[i]\n",
        "    data.at[i, \"Temperature in Montreal during episode\"] = Temp.loc[d[\"Year\"], d[\"Month\"], d[\"Day\"]].values[0]\n",
        "  return data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYawtIcAWi3m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = fill_NAs(data)\n",
        "test = fill_NAs(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bMnrY7Q2qQU",
        "colab_type": "code",
        "outputId": "50fc8157-be6f-4bc6-d502-3c4fc66d0fe2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        }
      },
      "source": [
        "print(data.isnull().sum())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode                                   0\n",
            "Station                                   0\n",
            "Channel Type                              0\n",
            "Season                                    0\n",
            "Year                                      0\n",
            "Day of week                               0\n",
            "Start_time                                0\n",
            "Length                                    0\n",
            "Genre                                     0\n",
            "First time or rerun                       0\n",
            "# of episode in the season                0\n",
            "Movie?                                    0\n",
            "Game of the Canadiens during episode?     0\n",
            "Market Share_total                        0\n",
            "Temperature in Montreal during episode    0\n",
            "Month                                     0\n",
            "Day                                       0\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yRn057rXHCC",
        "colab_type": "text"
      },
      "source": [
        "# Encode Episode Column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZC71kFm4YVo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_column_encoder(data, col_name, vocab_size=1000):\n",
        "  groups = pd.DataFrame(data[col_name]).groupby(col_name).groups\n",
        "  count = list(map(lambda x: (x, len(groups[x])), groups))\n",
        "  count = np.array(sorted(count, key = lambda x: x[1], reverse = True))\n",
        "  encoder = {}\n",
        "  encoder[\"OTHER\"] = 0\n",
        "  for i in range(vocab_size):\n",
        "    encoder[count[i][0]] = i+1\n",
        "  return encoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuYJi5WqcJfN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = 3000\n",
        "encoder = get_column_encoder(data, \"Episode\", vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHZ7GnJa-PNa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_col(encoder, data, col_name):\n",
        "  for i in range(data.shape[0]):\n",
        "    if data[col_name][i] in encoder:\n",
        "      data.at[i, col_name] = encoder[data[col_name][i]]\n",
        "    else:\n",
        "      data.at[i, col_name] = encoder[\"OTHER\"]\n",
        "  return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23bTPIRD-uT_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = encode_col(encoder, data, \"Episode\")\n",
        "test = encode_col(encoder, test, \"Episode\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUaH9i-Elg_C",
        "colab_type": "text"
      },
      "source": [
        "# Make dummies for categorical features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPQ8dJS9nEm9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data[\"Episode\"] = data[\"Episode\"].astype(int)\n",
        "test[\"Episode\"] = test[\"Episode\"].astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHyx65R9U_p4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ready_data = pd.get_dummies(data[data.select_dtypes(object).columns], drop_first=True).join(data.select_dtypes(np.number))\n",
        "ready_test = pd.get_dummies(test[test.select_dtypes(object).columns], drop_first=True).join(test.select_dtypes(np.number))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWf84QatkTzH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "missed_columns_in_test = ready_data.columns[ready_data.columns.isin(ready_test.columns) == False][:-1]\n",
        "for col in missed_columns_in_test:\n",
        "  ready_test = ready_test.join(pd.Series(np.zeros(test.shape[0]), name=col).astype(int))\n",
        "cols = list(ready_data.columns)\n",
        "del(cols[-3])\n",
        "ready_test = ready_test[cols]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ww6VF89BmtxP",
        "colab_type": "text"
      },
      "source": [
        "# Split data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29Zw03fAQEYe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ready_data = ready_data.sample(len(ready_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "safim66RRukx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = ready_data[:500000]\n",
        "valid_data = ready_data[500000:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xBqlFfjnhMA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_y = train_data[\"Market Share_total\"].values\n",
        "train_x_episode = train_data[ \"Episode\"].values\n",
        "train_x = train_data.drop([\"Market Share_total\", \"Episode\"], axis=1).values\n",
        "\n",
        "valid_y = valid_data[\"Market Share_total\"].values\n",
        "valid_x_episode = valid_data[ \"Episode\"].values\n",
        "valid_x = valid_data.drop([\"Market Share_total\", \"Episode\"], axis=1).values\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEYSC3j2iMec",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_x_episode = ready_test[ \"Episode\"].values\n",
        "test_x = ready_test.drop([\"Episode\"], axis=1).values\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvF9JGh2zRhW",
        "colab_type": "text"
      },
      "source": [
        "# Normalize Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCmejNSWm5On",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MU = []\n",
        "Sigma = []\n",
        "for i in range(train_x.shape[1]):\n",
        "  MU.append(train_x.T[i].mean())\n",
        "  Sigma.append(train_x.T[i].std())\n",
        "  valid_x.T[i] = (valid_x.T[i] - train_x.T[i].mean())/train_x.T[i].std()\n",
        "  train_x.T[i] = (train_x.T[i] - train_x.T[i].mean())/train_x.T[i].std()\n",
        "MU.append(train_y.mean())\n",
        "Sigma.append(train_y.std())\n",
        "valid_y = (valid_y - train_y.mean())/train_y.std()\n",
        "train_y = (train_y - train_y.mean())/train_y.std()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0k9faucjImE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(test_x.shape[1]):\n",
        "  test_x.T[i] = (test_x.T[i] - MU[i])/Sigma[i]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nciCGAdn8h9",
        "colab_type": "text"
      },
      "source": [
        "# A Naive model: Using Mean"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElWKuHlTnMHp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m = train_y.mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmCO2K6TeTVN",
        "colab_type": "code",
        "outputId": "346c696d-5738-47a6-e327-ba47b1d442c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"MAE of Using Mean is \", np.mean(np.abs(m - valid_y)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MAE of Using Mean is  0.5994806806208135\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3RHxyqUz0xG",
        "colab_type": "text"
      },
      "source": [
        "# A Simple model: Using Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TE2LRzkyeyti",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reg = LinearRegression().fit(train_x, train_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PqNiJdgxWCv",
        "colab_type": "code",
        "outputId": "68f92f85-af69-4b1d-bbaa-3929365b28d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"MAE of linear regression is \", np.mean(np.abs(reg.predict(valid_x) - valid_y)))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MAE of linear regression is  0.3642020818295221\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m28P-Oi41ZHq",
        "colab_type": "text"
      },
      "source": [
        "# Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1aXchilsgVj",
        "colab_type": "code",
        "outputId": "491f0055-89d7-4e6f-be3a-478b3f38ebfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        }
      },
      "source": [
        "model1 = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Input(shape=(1,)),\n",
        "  tf.keras.layers.Embedding(vocab_size+1, 16),\n",
        "  tf.keras.layers.Flatten()\n",
        "])\n",
        "\n",
        "model2 = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Input(shape=(81,)),\n",
        "  tf.keras.layers.Dense(120, activation='relu')\n",
        "])\n",
        "\n",
        "mergedOut = tf.keras.layers.Concatenate()([model1.output, model2.output])\n",
        "mergedOut = tf.keras.layers.Flatten()(mergedOut)\n",
        "mergedOut = tf.keras.layers.Dropout(0.2)(mergedOut)\n",
        "mergedOut = tf.keras.layers.Dense(120, activation='relu')(mergedOut)\n",
        "mergedOut = tf.keras.layers.Dropout(0.2)(mergedOut)\n",
        "mergedOut = tf.keras.layers.Dense(1)(mergedOut)\n",
        "\n",
        "model = tf.keras.models.Model([model1.input, model2.input], mergedOut)\n",
        "                              \n",
        "model.compile(loss='mae',\n",
        "        optimizer=\"Adam\",\n",
        "        metrics=[metrics.mae])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 1, 16)        48016       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 81)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 16)           0           embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 120)          9840        input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 136)          0           flatten[0][0]                    \n",
            "                                                                 dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 136)          0           concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 136)          0           flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 120)          16440       dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 120)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            121         dropout_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 74,417\n",
            "Trainable params: 74,417\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oJaanFErlqO",
        "colab_type": "code",
        "outputId": "c394e142-6196-4987-cbbc-268ed2679006",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "epochs = 50\n",
        "batch_size = 128\n",
        "history = model.fit([train_x_episode, train_x], train_y,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    shuffle=True,\n",
        "    verbose=1, # Change it to 2, if wished to observe execution\n",
        "    validation_data=([valid_x_episode, valid_x], valid_y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 500000 samples, validate on 116656 samples\n",
            "Epoch 1/50\n",
            "500000/500000 [==============================] - 26s 53us/sample - loss: 0.2752 - mean_absolute_error: 0.2752 - val_loss: 0.2417 - val_mean_absolute_error: 0.2417\n",
            "Epoch 2/50\n",
            "500000/500000 [==============================] - 25s 50us/sample - loss: 0.2445 - mean_absolute_error: 0.2445 - val_loss: 0.2295 - val_mean_absolute_error: 0.2295\n",
            "Epoch 3/50\n",
            "500000/500000 [==============================] - 25s 50us/sample - loss: 0.2373 - mean_absolute_error: 0.2373 - val_loss: 0.2233 - val_mean_absolute_error: 0.2233\n",
            "Epoch 4/50\n",
            "500000/500000 [==============================] - 25s 51us/sample - loss: 0.2328 - mean_absolute_error: 0.2328 - val_loss: 0.2218 - val_mean_absolute_error: 0.2218\n",
            "Epoch 5/50\n",
            "500000/500000 [==============================] - 25s 50us/sample - loss: 0.2305 - mean_absolute_error: 0.2305 - val_loss: 0.2181 - val_mean_absolute_error: 0.2181\n",
            "Epoch 6/50\n",
            "500000/500000 [==============================] - 25s 50us/sample - loss: 0.2283 - mean_absolute_error: 0.2283 - val_loss: 0.2191 - val_mean_absolute_error: 0.2191\n",
            "Epoch 7/50\n",
            "500000/500000 [==============================] - 25s 50us/sample - loss: 0.2270 - mean_absolute_error: 0.2270 - val_loss: 0.2158 - val_mean_absolute_error: 0.2158\n",
            "Epoch 8/50\n",
            "500000/500000 [==============================] - 26s 51us/sample - loss: 0.2256 - mean_absolute_error: 0.2256 - val_loss: 0.2142 - val_mean_absolute_error: 0.2142\n",
            "Epoch 9/50\n",
            "500000/500000 [==============================] - 25s 50us/sample - loss: 0.2245 - mean_absolute_error: 0.2245 - val_loss: 0.2144 - val_mean_absolute_error: 0.2144\n",
            "Epoch 10/50\n",
            "500000/500000 [==============================] - 26s 52us/sample - loss: 0.2238 - mean_absolute_error: 0.2238 - val_loss: 0.2184 - val_mean_absolute_error: 0.2184\n",
            "Epoch 11/50\n",
            "500000/500000 [==============================] - 25s 49us/sample - loss: 0.2228 - mean_absolute_error: 0.2228 - val_loss: 0.2137 - val_mean_absolute_error: 0.2137\n",
            "Epoch 12/50\n",
            "500000/500000 [==============================] - 25s 50us/sample - loss: 0.2226 - mean_absolute_error: 0.2226 - val_loss: 0.2109 - val_mean_absolute_error: 0.2109\n",
            "Epoch 13/50\n",
            "500000/500000 [==============================] - 25s 49us/sample - loss: 0.2214 - mean_absolute_error: 0.2214 - val_loss: 0.2129 - val_mean_absolute_error: 0.2129\n",
            "Epoch 14/50\n",
            "500000/500000 [==============================] - 25s 49us/sample - loss: 0.2210 - mean_absolute_error: 0.2210 - val_loss: 0.2094 - val_mean_absolute_error: 0.2094\n",
            "Epoch 15/50\n",
            "500000/500000 [==============================] - 25s 49us/sample - loss: 0.2202 - mean_absolute_error: 0.2202 - val_loss: 0.2143 - val_mean_absolute_error: 0.2143\n",
            "Epoch 16/50\n",
            "500000/500000 [==============================] - 25s 50us/sample - loss: 0.2201 - mean_absolute_error: 0.2201 - val_loss: 0.2118 - val_mean_absolute_error: 0.2118\n",
            "Epoch 17/50\n",
            "500000/500000 [==============================] - 25s 50us/sample - loss: 0.2196 - mean_absolute_error: 0.2196 - val_loss: 0.2103 - val_mean_absolute_error: 0.2103\n",
            "Epoch 18/50\n",
            "500000/500000 [==============================] - 24s 49us/sample - loss: 0.2192 - mean_absolute_error: 0.2192 - val_loss: 0.2088 - val_mean_absolute_error: 0.2088\n",
            "Epoch 19/50\n",
            "500000/500000 [==============================] - 24s 49us/sample - loss: 0.2189 - mean_absolute_error: 0.2189 - val_loss: 0.2109 - val_mean_absolute_error: 0.2109\n",
            "Epoch 20/50\n",
            "500000/500000 [==============================] - 25s 51us/sample - loss: 0.2186 - mean_absolute_error: 0.2186 - val_loss: 0.2091 - val_mean_absolute_error: 0.2091\n",
            "Epoch 21/50\n",
            "500000/500000 [==============================] - 25s 49us/sample - loss: 0.2183 - mean_absolute_error: 0.2183 - val_loss: 0.2103 - val_mean_absolute_error: 0.2103\n",
            "Epoch 22/50\n",
            "500000/500000 [==============================] - 25s 49us/sample - loss: 0.2178 - mean_absolute_error: 0.2178 - val_loss: 0.2074 - val_mean_absolute_error: 0.2074\n",
            "Epoch 23/50\n",
            "500000/500000 [==============================] - 25s 49us/sample - loss: 0.2176 - mean_absolute_error: 0.2176 - val_loss: 0.2085 - val_mean_absolute_error: 0.2085\n",
            "Epoch 24/50\n",
            "500000/500000 [==============================] - 25s 49us/sample - loss: 0.2172 - mean_absolute_error: 0.2172 - val_loss: 0.2065 - val_mean_absolute_error: 0.2065\n",
            "Epoch 25/50\n",
            "500000/500000 [==============================] - 25s 50us/sample - loss: 0.2171 - mean_absolute_error: 0.2171 - val_loss: 0.2071 - val_mean_absolute_error: 0.2071\n",
            "Epoch 26/50\n",
            "500000/500000 [==============================] - 25s 49us/sample - loss: 0.2169 - mean_absolute_error: 0.2169 - val_loss: 0.2092 - val_mean_absolute_error: 0.2092\n",
            "Epoch 27/50\n",
            "500000/500000 [==============================] - 25s 50us/sample - loss: 0.2166 - mean_absolute_error: 0.2166 - val_loss: 0.2066 - val_mean_absolute_error: 0.2066\n",
            "Epoch 28/50\n",
            "500000/500000 [==============================] - 25s 51us/sample - loss: 0.2164 - mean_absolute_error: 0.2164 - val_loss: 0.2079 - val_mean_absolute_error: 0.2079\n",
            "Epoch 29/50\n",
            "500000/500000 [==============================] - 25s 50us/sample - loss: 0.2162 - mean_absolute_error: 0.2162 - val_loss: 0.2089 - val_mean_absolute_error: 0.2089\n",
            "Epoch 30/50\n",
            "500000/500000 [==============================] - 25s 49us/sample - loss: 0.2161 - mean_absolute_error: 0.2161 - val_loss: 0.2068 - val_mean_absolute_error: 0.2068\n",
            "Epoch 31/50\n",
            "500000/500000 [==============================] - 25s 49us/sample - loss: 0.2156 - mean_absolute_error: 0.2156 - val_loss: 0.2059 - val_mean_absolute_error: 0.2059\n",
            "Epoch 32/50\n",
            "500000/500000 [==============================] - 25s 50us/sample - loss: 0.2153 - mean_absolute_error: 0.2153 - val_loss: 0.2091 - val_mean_absolute_error: 0.2091\n",
            "Epoch 33/50\n",
            "500000/500000 [==============================] - 26s 52us/sample - loss: 0.2155 - mean_absolute_error: 0.2155 - val_loss: 0.2069 - val_mean_absolute_error: 0.2069\n",
            "Epoch 34/50\n",
            "500000/500000 [==============================] - 25s 50us/sample - loss: 0.2152 - mean_absolute_error: 0.2152 - val_loss: 0.2066 - val_mean_absolute_error: 0.2066\n",
            "Epoch 35/50\n",
            "500000/500000 [==============================] - 25s 50us/sample - loss: 0.2155 - mean_absolute_error: 0.2155 - val_loss: 0.2055 - val_mean_absolute_error: 0.2055\n",
            "Epoch 36/50\n",
            "500000/500000 [==============================] - 25s 50us/sample - loss: 0.2147 - mean_absolute_error: 0.2147 - val_loss: 0.2098 - val_mean_absolute_error: 0.2098\n",
            "Epoch 37/50\n",
            "500000/500000 [==============================] - 25s 50us/sample - loss: 0.2148 - mean_absolute_error: 0.2148 - val_loss: 0.2047 - val_mean_absolute_error: 0.2047\n",
            "Epoch 38/50\n",
            "500000/500000 [==============================] - 25s 50us/sample - loss: 0.2147 - mean_absolute_error: 0.2147 - val_loss: 0.2072 - val_mean_absolute_error: 0.2072\n",
            "Epoch 39/50\n",
            "500000/500000 [==============================] - 25s 49us/sample - loss: 0.2145 - mean_absolute_error: 0.2145 - val_loss: 0.2114 - val_mean_absolute_error: 0.2114\n",
            "Epoch 40/50\n",
            "500000/500000 [==============================] - 25s 50us/sample - loss: 0.2144 - mean_absolute_error: 0.2144 - val_loss: 0.2081 - val_mean_absolute_error: 0.2081\n",
            "Epoch 41/50\n",
            "500000/500000 [==============================] - 26s 51us/sample - loss: 0.2145 - mean_absolute_error: 0.2145 - val_loss: 0.2066 - val_mean_absolute_error: 0.2066\n",
            "Epoch 42/50\n",
            "500000/500000 [==============================] - 25s 49us/sample - loss: 0.2143 - mean_absolute_error: 0.2143 - val_loss: 0.2054 - val_mean_absolute_error: 0.2054\n",
            "Epoch 43/50\n",
            "500000/500000 [==============================] - 24s 49us/sample - loss: 0.2142 - mean_absolute_error: 0.2142 - val_loss: 0.2052 - val_mean_absolute_error: 0.2052\n",
            "Epoch 44/50\n",
            "500000/500000 [==============================] - 24s 49us/sample - loss: 0.2143 - mean_absolute_error: 0.2143 - val_loss: 0.2056 - val_mean_absolute_error: 0.2056\n",
            "Epoch 45/50\n",
            "500000/500000 [==============================] - 25s 51us/sample - loss: 0.2140 - mean_absolute_error: 0.2140 - val_loss: 0.2039 - val_mean_absolute_error: 0.2039\n",
            "Epoch 46/50\n",
            "500000/500000 [==============================] - 25s 50us/sample - loss: 0.2137 - mean_absolute_error: 0.2137 - val_loss: 0.2048 - val_mean_absolute_error: 0.2048\n",
            "Epoch 47/50\n",
            "500000/500000 [==============================] - 25s 49us/sample - loss: 0.2137 - mean_absolute_error: 0.2137 - val_loss: 0.2074 - val_mean_absolute_error: 0.2074\n",
            "Epoch 48/50\n",
            "500000/500000 [==============================] - 24s 49us/sample - loss: 0.2137 - mean_absolute_error: 0.2137 - val_loss: 0.2070 - val_mean_absolute_error: 0.2070\n",
            "Epoch 49/50\n",
            "500000/500000 [==============================] - 25s 49us/sample - loss: 0.2137 - mean_absolute_error: 0.2137 - val_loss: 0.2074 - val_mean_absolute_error: 0.2074\n",
            "Epoch 50/50\n",
            "500000/500000 [==============================] - 25s 49us/sample - loss: 0.2136 - mean_absolute_error: 0.2136 - val_loss: 0.2071 - val_mean_absolute_error: 0.2071\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGR8A-D6_foj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict([valid_x_episode, valid_x])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-87NE36V_3Kg",
        "colab_type": "code",
        "outputId": "e67b047e-b3b1-429a-d404-82e9074b9ddf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"MAE of NN is \", np.mean(np.abs(y_pred.flatten() - valid_y)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MAE of NN is  0.20351906403968664\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myNWz13qALnI",
        "colab_type": "code",
        "outputId": "74f60571-a08c-4e74-ddac-dd56a8364169",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"R2 score of NN is \", r2_score(valid_y, y_pred.flatten()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "R2 score of NN is  0.8782284809241947\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NFaegYahhiW",
        "colab_type": "text"
      },
      "source": [
        "# Predict "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtE12LMLhQvS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred_test = model.predict([test_x_episode, test_x])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFoZ1wdGqfmv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.DataFrame(y_pred * Sigma[-1] + MU[-1]).to_csv(\"drive/My Drive/pred_test.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIePKhOjqqNk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}